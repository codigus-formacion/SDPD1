{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL: Stack Exchange SciFi community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Exchange is a [network of Q&A communities](http://stackexchange.com/sites#), covering a wide range of topics. Its first instance, Stack Overflow is today one of the most prolific examples of collaborative knowledge creation in Internet. [Science Fiction and Fantasy (SciFi)](http://scifi.stackexchange.com/) in one of such communities, focused on questions on science fiction and fantasy.\n",
    "\n",
    "Stack Exchange also follows an open data approach regarding the activity records collected from all its sites. The [Data Explorer Service](https://data.stackexchange.com/) allows users to query activity datasets from any community interactively. In addition, the organization [regularly publishes](https://archive.org/details/stackexchange) a full dump (torrent of files) including the whole set of activity log records from all sites, for offline data analysis.\n",
    "\n",
    "In this example, we load and query a excerpt of one of such files in XML format to illustrate Spark SQL features. To read XML files in Spark we make use of an external library, the [XML Data Source for Apache Spark](https://github.com/databricks/spark-xml) developed by Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we must launch `pyspark` with the **`--packages com.databricks:spark-xml_2.12:0.14.0`** option, so that Jupyter loads this dependency when we open the notebook.\n",
    "\n",
    "**IMPORTANT**: Please note that, **depending on your specific version of Spark**, you must choose a compatible version of `spark-xml` that also matches the Scala version used to compile it. **Please, refer to the [summary table available on the project page in GitHub](https://github.com/databricks/spark-xml#requirements) to select the appropriate combination for your platform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external packages programatically\n",
    "# Here, we assume that you use Spark 3.2.1 or later (compiled against Scala 2.12)\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "\n",
    "packages = \"com.databricks:spark-xml_2.12:0.16.0\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/jvm/java-17-openjdk-amd64'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"JAVA_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--packages com.databricks:spark-xml_2.12:0.16.0 pyspark-shell'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 12:36:48 WARN Utils: Your hostname, helium resolves to a loopback address: 127.0.1.1; using 10.6.36.17 instead (on interface wlo1)\n",
      "23/03/25 12:36:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/jfelipe/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jfelipe/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jfelipe/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3b353100-e5cf-4dbf-9079-d5bc40d8fb73;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.16.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 237ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.16.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3b353100-e5cf-4dbf-9079-d5bc40d8fb73\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 12:36:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.cores\", 1)\n",
    "    .appName(\"StackExchange SciFi\")\n",
    "    .getOrCreate() )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions\n",
    "# Define data scheme\n",
    "# Each XML element in the tree is identified by its tag\n",
    "# We can use @attr_name to refer to an attribute of an element\n",
    "customSchema = StructType([\n",
    "                    StructField(\"AccountId\", LongType(), True), \n",
    "                    StructField(\"Age\", ByteType(), True),\n",
    "                    StructField(\"CreationDate\", TimestampType(), True),\n",
    "                    StructField(\"DisplayName\", StringType(), True),\n",
    "                    StructField(\"DownVotes\", IntegerType(), True),\n",
    "                    StructField(\"Id\", LongType(), True),\n",
    "                    StructField(\"LastAccessDate\", TimestampType(), True),\n",
    "                    StructField(\"Location\", StringType(), True),\n",
    "                    StructField(\"ProfileImageUrl\", StringType(), True),\n",
    "                    StructField(\"Reputation\", IntegerType(), True),\n",
    "                    StructField(\"UpVotes\", IntegerType(), True),\n",
    "                    StructField(\"Views\", IntegerType(), True),\n",
    "                    StructField(\"WebsiteUrl\", StringType(), True)\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct data loading from XML file\n",
    "df = (spark.read.format('xml')\n",
    "                     .options(rowTag='row')\n",
    "                     .load('../data/Users_conv2.xml',\n",
    "                           schema = customSchema)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29831"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of users in this excerpt from complete SciFi dump\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AccountId: long (nullable = true)\n",
      " |-- Age: byte (nullable = true)\n",
      " |-- CreationDate: timestamp (nullable = true)\n",
      " |-- DisplayName: string (nullable = true)\n",
      " |-- DownVotes: integer (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- LastAccessDate: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- ProfileImageUrl: string (nullable = true)\n",
      " |-- Reputation: integer (nullable = true)\n",
      " |-- UpVotes: integer (nullable = true)\n",
      " |-- Views: integer (nullable = true)\n",
      " |-- WebsiteUrl: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame content scheme\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AccountId=-1, Age=None, CreationDate=datetime.datetime(2011, 1, 11, 20, 19, 36, 483000), DisplayName='Community', DownVotes=3953, Id=-1, LastAccessDate=datetime.datetime(2011, 1, 11, 20, 19, 36, 483000), Location='on the server farm', ProfileImageUrl=None, Reputation=1, UpVotes=2587, Views=0, WebsiteUrl='http://meta.stackexchange.com/'),\n",
       " Row(AccountId=2, Age=38, CreationDate=datetime.datetime(2011, 1, 11, 20, 50, 40, 620000), DisplayName='Geoff Dalgas', DownVotes=0, Id=2, LastAccessDate=datetime.datetime(2015, 2, 5, 1, 3, 28, 30000), Location='Corvallis, OR', ProfileImageUrl=None, Reputation=101, UpVotes=1, Views=21, WebsiteUrl='http://stackoverflow.com'),\n",
       " Row(AccountId=7598, Age=30, CreationDate=datetime.datetime(2011, 1, 11, 20, 55, 41, 460000), DisplayName='Nick Craver', DownVotes=0, Id=3, LastAccessDate=datetime.datetime(2015, 3, 1, 14, 49, 49, 173000), Location='Winston-Salem, NC', ProfileImageUrl='http://i.stack.imgur.com/nGCYr.jpg', Reputation=101, UpVotes=3, Views=10, WebsiteUrl='http://nickcraver.com/blog/'),\n",
       " Row(AccountId=1998, Age=29, CreationDate=datetime.datetime(2011, 1, 11, 21, 17, 1, 887000), DisplayName='Emmett', DownVotes=0, Id=4, LastAccessDate=datetime.datetime(2013, 5, 6, 22, 52, 29, 970000), Location='San Francisco, CA', ProfileImageUrl='http://i.stack.imgur.com/d1oHX.jpg', Reputation=101, UpVotes=0, Views=7, WebsiteUrl='http://minesweeperonline.com'),\n",
       " Row(AccountId=29738, Age=None, CreationDate=datetime.datetime(2011, 1, 11, 21, 18, 52, 493000), DisplayName='Kevin Montrose', DownVotes=0, Id=5, LastAccessDate=datetime.datetime(2015, 2, 15, 6, 27, 23, 23000), Location='New York City, New York', ProfileImageUrl=None, Reputation=101, UpVotes=32, Views=11, WebsiteUrl='http://kevinmontrose.com/'),\n",
       " Row(AccountId=32917, Age=30, CreationDate=datetime.datetime(2011, 1, 11, 21, 29, 26, 230000), DisplayName='David Fullerton', DownVotes=4, Id=6, LastAccessDate=datetime.datetime(2015, 3, 6, 23, 57, 46, 977000), Location='New York, NY', ProfileImageUrl=None, Reputation=99, UpVotes=63, Views=13, WebsiteUrl='http://careers.stackoverflow.com/dfullerton')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.6.36.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>StackExchange SciFi</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f07247563b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register temp table for SQL queries (not required in newer Spark versions)\n",
    "# df.registerTempTable('scifi')\n",
    "df.createOrReplaceTempView('scifi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| Id|   DisplayName|\n",
      "+---+--------------+\n",
      "|300|       BozoJoe|\n",
      "|301|  Neil Trodden|\n",
      "|302|        Uberto|\n",
      "|304|   Nick Haslam|\n",
      "|305|           rmx|\n",
      "|307|       Nerevar|\n",
      "|309|      Uwe Keim|\n",
      "|310|Wouter Lievens|\n",
      "|311| Tim Schmelter|\n",
      "|312|           nik|\n",
      "+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all users (Id, DisplayName) based in San Francisco (using simple regexp)\n",
    "loc_SFco = spark.sql(\"\"\"\n",
    "SELECT Id, DisplayName\n",
    "FROM scifi\n",
    "WHERE (Id >= 300)\n",
    "\"\"\")\n",
    "loc_SFco.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_SFco.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------------------+\n",
      "|  Id|       DisplayName|            Location|\n",
      "+----+------------------+--------------------+\n",
      "| 250|     John Saunders|    Phoenix, AZ, USA|\n",
      "| 558|           JYelton|           Utah, USA|\n",
      "|1205|          Peter K.|             CT, USA|\n",
      "|1293|       Dan Herbert|San Francisco Bay...|\n",
      "|1397|         gallamine|             NC, USA|\n",
      "|1653|          zemoxian|Silver Spring, MD...|\n",
      "|1693|             Tango|   Richmond, VA, USA|\n",
      "|2033|        Thom Blake|  New Haven, CT, USA|\n",
      "|2529|           AruniRC|                 USA|\n",
      "|2545|             gef05| North Carolina, USA|\n",
      "|2615|             Alger|             NJ, USA|\n",
      "|3584|          Unsigned|     West Coast, USA|\n",
      "|3603|         Thomas W.| Pittsburgh, PA, USA|\n",
      "|3757|   Brian Wigginton|     Austin, TX, USA|\n",
      "|3850|               YHZ|                 USA|\n",
      "|4102|         ryuuyasha|                  US|\n",
      "|4133|               JNK|             CT, USA|\n",
      "|4407|    Adrian Cornish|                 USA|\n",
      "|4524|Franck Dernoncourt| MIT, Cambridge, USA|\n",
      "|4617|  DreadPirateShawn|Kirkland, WA, USA...|\n",
      "|4678|           SSumner|                 USA|\n",
      "|4899|      Matt Fenwick|                 USA|\n",
      "|5267|     Robin Salazar|           Utah, USA|\n",
      "|5553|     David Pointer|Colorado Springs,...|\n",
      "|5686|      Buck Calabro|     Upstate NY, USA|\n",
      "|6015|    Mihai Maruseac|     Boston, MA, USA|\n",
      "|6157|       jeffreylees|           Ohio, USA|\n",
      "|6788|        Force Flow|    Northeastern USA|\n",
      "|6853|     ThinkingStiff|     Seattle, WA USA|\n",
      "|9367|            jbabey|     Connecticut USA|\n",
      "+----+------------------+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all users (Id, DisplayName) based in San Francisco \n",
    "# (using simple regexp)\n",
    "loc_US = spark.sql(\"\"\"\n",
    "SELECT Id, DisplayName, Location\n",
    "FROM scifi\n",
    "WHERE Location LIKE '%USA%' OR Location LIKE '%US%' OR Location LIKE 'USA%'\n",
    "                            OR Location LIKE 'US%'\n",
    "\"\"\")\n",
    "loc_US.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_US.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|Min_Age|Max_Age|\n",
      "+-------+-------+\n",
      "|     13|     95|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find the min and max age of all users who created a new account on March 2014 or later\n",
    "users_after_210403 = spark.sql(\"\"\"\n",
    "SELECT MIN(Age) AS Min_Age, MAX(Age) AS Max_Age\n",
    "FROM scifi\n",
    "WHERE CreationDate > '2014-03-01'\n",
    "\"\"\")\n",
    "users_after_210403.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|max(Reputation)|\n",
      "+---------------+\n",
      "|         148259|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the user with the max. reputation in this excerpt\n",
    "max_reputation = spark.sql(\"\"\"\n",
    "SELECT MAX(Reputation) FROM scifi\n",
    "\"\"\")\n",
    "max_reputation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+\n",
      "| Id|DisplayName|UpVotes|DownVotes|\n",
      "+---+-----------+-------+---------+\n",
      "|976|        DVK|   5503|     1806|\n",
      "+---+-----------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display metadata for that user with max. reputation\n",
    "user_max_reputation = spark.sql(\"\"\"\n",
    "SELECT Id, DisplayName, UpVotes, DownVotes FROM scifi\n",
    "WHERE Reputation = 148259\n",
    "\"\"\")\n",
    "user_max_reputation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+---------+\n",
      "| Id|DisplayName|UpVotes|DownVotes|\n",
      "+---+-----------+-------+---------+\n",
      "|976|        DVK|   5503|     1806|\n",
      "+---+-----------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_max_reputation = spark.sql(\"\"\"\n",
    "SELECT Id, DisplayName, UpVotes, DownVotes FROM scifi\n",
    "WHERE Reputation = (SELECT MAX(Reputation) FROM scifi)\n",
    "\"\"\")\n",
    "user_max_reputation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to stop SparkContext before shutting down this notebook\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
